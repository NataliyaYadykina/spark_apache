{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Задание\n\nУсловие: используйте источник rate, напишите код, который создаст дополнительный столбец, который будет выводить сумму только нечётных чисел.","metadata":{}},{"cell_type":"code","source":"!pip install pyspark >> None","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:55:11.837931Z","iopub.execute_input":"2024-05-03T10:55:11.838809Z","iopub.status.idle":"2024-05-03T10:56:07.885135Z","shell.execute_reply.started":"2024-05-03T10:55:11.838760Z","shell.execute_reply":"2024-05-03T10:56:07.883911Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum\nfrom pyspark.sql.functions import *","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:56:46.432401Z","iopub.execute_input":"2024-05-03T10:56:46.434530Z","iopub.status.idle":"2024-05-03T10:56:46.549233Z","shell.execute_reply.started":"2024-05-03T10:56:46.434448Z","shell.execute_reply":"2024-05-03T10:56:46.547991Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Исходные данные выведем в консоль\nspark = SparkSession.builder.appName(\"SourceElements\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\nquery = df.writeStream.outputMode(\"append\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:57:31.882944Z","iopub.execute_input":"2024-05-03T10:57:31.883944Z","iopub.status.idle":"2024-05-03T10:57:51.703996Z","shell.execute_reply.started":"2024-05-03T10:57:31.883902Z","shell.execute_reply":"2024-05-03T10:57:51.702587Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/03 10:57:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/05/03 10:57:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2a9b83a7-0366-438c-bc0b-817a32696c61. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 10:57:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    0|\n|2024-05-03 10:57:...|    1|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    2|\n|2024-05-03 10:57:...|    3|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    4|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    5|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    6|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    7|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 10:57:...|    8|\n+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Отфильтруем только нечетные данные, просуммируем, сохраним в новый столбец sum_odd, выведем результат\nspark = SparkSession.builder.appName(\"SumOddNumbers\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\ndf_odd = df.filter(\"value % 2 == 1\")\ndf_sum = df_odd.selectExpr(\"sum(value) AS sum_odd\")\nquery = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T10:59:43.078466Z","iopub.execute_input":"2024-05-03T10:59:43.079041Z","iopub.status.idle":"2024-05-03T10:59:53.424609Z","shell.execute_reply.started":"2024-05-03T10:59:43.078999Z","shell.execute_reply":"2024-05-03T10:59:53.422983Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"24/05/03 10:59:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n24/05/03 10:59:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ec3d97cf-5794-4f64-8c30-0d0bf1a4417d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 10:59:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|   NULL|\n+-------+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|   NULL|\n+-------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      1|\n+-------+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      1|\n+-------+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      4|\n+-------+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      4|\n+-------+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      9|\n+-------+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      9|\n+-------+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|     16|\n+-------+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|     16|\n+-------+\n\n","output_type":"stream"}]}]}