{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Задание\n\nУсловие: используйте источник rate, напишите код, который создаст дополнительный столбец, который будет выводить сумму только нечётных чисел.","metadata":{}},{"cell_type":"code","source":"!pip install pyspark >> None","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:05:09.710270Z","iopub.execute_input":"2024-05-03T11:05:09.710640Z","iopub.status.idle":"2024-05-03T11:06:07.795935Z","shell.execute_reply.started":"2024-05-03T11:05:09.710611Z","shell.execute_reply":"2024-05-03T11:06:07.793858Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum\nfrom pyspark.sql.functions import *","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:06:12.275547Z","iopub.execute_input":"2024-05-03T11:06:12.275950Z","iopub.status.idle":"2024-05-03T11:06:12.385101Z","shell.execute_reply.started":"2024-05-03T11:06:12.275916Z","shell.execute_reply":"2024-05-03T11:06:12.383926Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Исходные данные выведем в консоль\nspark = SparkSession.builder.appName(\"SourceElements\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\nquery = df.writeStream.outputMode(\"append\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:06:33.695244Z","iopub.execute_input":"2024-05-03T11:06:33.701302Z","iopub.status.idle":"2024-05-03T11:06:56.507785Z","shell.execute_reply.started":"2024-05-03T11:06:33.701117Z","shell.execute_reply":"2024-05-03T11:06:56.506435Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/03 11:06:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/05/03 11:06:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a8126c9a-1223-426f-bfa6-279fe8a0f9b0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 11:06:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    0|\n|2024-05-03 11:06:...|    1|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    2|\n|2024-05-03 11:06:...|    3|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    4|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    5|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    6|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    7|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 11:06:...|    8|\n+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Отфильтруем только нечетные данные, просуммируем, сохраним в новый столбец sum_odd, выведем результат\ndf_odd = df.filter(\"value % 2 == 1\")\ndf_sum = df_odd.selectExpr(\"sum(value) AS sum_odd\")\nquery = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:07:34.047953Z","iopub.execute_input":"2024-05-03T11:07:34.048427Z","iopub.status.idle":"2024-05-03T11:07:44.291711Z","shell.execute_reply.started":"2024-05-03T11:07:34.048388Z","shell.execute_reply":"2024-05-03T11:07:44.290351Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"24/05/03 11:07:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-819064a1-bdcb-45e8-9e2e-f943ed5c2c5c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 11:07:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|   NULL|\n+-------+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      1|\n+-------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      1|\n+-------+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      4|\n+-------+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      4|\n+-------+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      9|\n+-------+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|      9|\n+-------+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|     16|\n+-------+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+-------+\n|sum_odd|\n+-------+\n|     16|\n+-------+\n\n","output_type":"stream"}]}]}