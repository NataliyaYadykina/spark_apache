{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-03T08:52:36.548939Z","iopub.execute_input":"2024-05-03T08:52:36.549980Z","iopub.status.idle":"2024-05-03T08:52:37.735711Z","shell.execute_reply.started":"2024-05-03T08:52:36.549939Z","shell.execute_reply":"2024-05-03T08:52:37.734766Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark >> None","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:53:58.537566Z","iopub.execute_input":"2024-05-03T08:53:58.538024Z","iopub.status.idle":"2024-05-03T08:54:46.930128Z","shell.execute_reply.started":"2024-05-03T08:53:58.537990Z","shell.execute_reply":"2024-05-03T08:54:46.928958Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum\nfrom pyspark.sql.functions import *","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:55:10.932036Z","iopub.execute_input":"2024-05-03T08:55:10.932415Z","iopub.status.idle":"2024-05-03T08:55:10.937757Z","shell.execute_reply.started":"2024-05-03T08:55:10.932387Z","shell.execute_reply":"2024-05-03T08:55:10.936919Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"CountElements\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\nquery = df.writeStream.outputMode(\"append\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:55:32.694139Z","iopub.execute_input":"2024-05-03T08:55:32.694544Z","iopub.status.idle":"2024-05-03T08:55:51.130828Z","shell.execute_reply.started":"2024-05-03T08:55:32.694514Z","shell.execute_reply":"2024-05-03T08:55:51.129721Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/03 08:55:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n24/05/03 08:55:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cf0ad840-4760-4151-9ec4-f52496fe598a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 08:55:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    0|\n|2024-05-03 08:55:...|    1|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    2|\n|2024-05-03 08:55:...|    3|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    4|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    5|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    6|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    7|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 08:55:...|    8|\n+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"FilterEvenNumbers\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\ndf_even = df.filter(\"value % 2 == 0\")\nquery = df_even.writeStream.outputMode(\"append\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:02:19.235782Z","iopub.execute_input":"2024-05-03T09:02:19.236225Z","iopub.status.idle":"2024-05-03T09:02:29.479116Z","shell.execute_reply.started":"2024-05-03T09:02:19.236189Z","shell.execute_reply":"2024-05-03T09:02:29.478038Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"24/05/03 09:02:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n24/05/03 09:02:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8c98dfbc-0ebd-4a41-90f2-3f7f83b1f9bf. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 09:02:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 09:02:...|    0|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 09:02:...|    2|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 09:02:...|    4|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 09:02:...|    6|\n+--------------------+-----+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+--------------------+-----+\n|           timestamp|value|\n+--------------------+-----+\n|2024-05-03 09:02:...|    8|\n+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"GroupByValue\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\ndf_grouped = df.groupBy(\"value\").count()\nquery = df_grouped.writeStream.outputMode(\"update\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:21:08.385599Z","iopub.execute_input":"2024-05-03T09:21:08.386006Z","iopub.status.idle":"2024-05-03T09:21:18.543622Z","shell.execute_reply.started":"2024-05-03T09:21:08.385977Z","shell.execute_reply":"2024-05-03T09:21:18.542461Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"24/05/03 09:21:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-63da78a1-b6eb-4327-ba31-2d1a3a4405f5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 09:21:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n+-----+-----+\n\n","output_type":"stream"},{"name":"stderr","text":"24/05/03 09:21:18 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n24/05/03 09:21:18 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n24/05/03 09:21:18 WARN Shell: Interrupted while joining on: Thread[Thread-9752,5,main]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:21:18 WARN Shell: Interrupted while joining on: Thread[Thread-9753,5,main]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:21:18 WARN Shell: Interrupted while joining on: Thread[Thread-9754,5,]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:21:18 WARN TaskSetManager: Lost task 156.0 in stage 27.0 (TID 926) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 22 cancelled part of cancelled job group 0e09c571-458e-46e4-9e6c-3b859bd0c67e)\n24/05/03 09:21:18 WARN TaskSetManager: Lost task 157.0 in stage 27.0 (TID 927) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 22 cancelled part of cancelled job group 0e09c571-458e-46e4-9e6c-3b859bd0c67e)\n24/05/03 09:21:18 ERROR Utils: Aborting task\njava.lang.IllegalStateException: Error committing version 2 into HDFSStateStore[id=(op=0,part=158),dir=file:/tmp/temporary-63da78a1-b6eb-4327-ba31-2d1a3a4405f5/state/0/158]\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: File file:/tmp/temporary-63da78a1-b6eb-4327-ba31-2d1a3a4405f5/state/0/158 does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\t... 31 more\n24/05/03 09:21:18 ERROR DataWritingSparkTask: Aborting commit for partition 158 (task 928, attempt 0, stage 27.0)\n24/05/03 09:21:18 ERROR DataWritingSparkTask: Aborted commit for partition 158 (task 928, attempt 0, stage 27.0)\n24/05/03 09:21:18 WARN TaskSetManager: Lost task 158.0 in stage 27.0 (TID 928) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 22 cancelled part of cancelled job group 0e09c571-458e-46e4-9e6c-3b859bd0c67e)\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"SumValues\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\ndf_sum = df.selectExpr(\"sum(value) AS total\")\nquery = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:22:18.288793Z","iopub.execute_input":"2024-05-03T09:22:18.289174Z","iopub.status.idle":"2024-05-03T09:22:28.460546Z","shell.execute_reply.started":"2024-05-03T09:22:18.289147Z","shell.execute_reply":"2024-05-03T09:22:28.459225Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"24/05/03 09:22:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n24/05/03 09:22:18 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-863f65cb-6660-43f7-be29-c6b5d2d58f51. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 09:22:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----+\n|total|\n+-----+\n| NULL|\n+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|    0|\n+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|    1|\n+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|    3|\n+-----+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|    6|\n+-----+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|   10|\n+-----+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|   15|\n+-----+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|   21|\n+-----+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|   28|\n+-----+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+-----+\n|total|\n+-----+\n|   36|\n+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"MaxValue\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\ndf_max = df.agg(max('value'))\nquery = df_max.writeStream.outputMode(\"update\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:24:15.030483Z","iopub.execute_input":"2024-05-03T09:24:15.031638Z","iopub.status.idle":"2024-05-03T09:24:25.129598Z","shell.execute_reply.started":"2024-05-03T09:24:15.031594Z","shell.execute_reply":"2024-05-03T09:24:25.128751Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"24/05/03 09:24:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n24/05/03 09:24:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-47036bbc-6ee6-4aa7-89ce-9f8632da9ee7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 09:24:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|      NULL|\n+----------+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         0|\n+----------+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         1|\n+----------+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         2|\n+----------+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         3|\n+----------+\n\n-------------------------------------------\nBatch: 5\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         4|\n+----------+\n\n-------------------------------------------\nBatch: 6\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         5|\n+----------+\n\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         6|\n+----------+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         7|\n+----------+\n\n-------------------------------------------\nBatch: 9\n-------------------------------------------\n+----------+\n|max(value)|\n+----------+\n|         8|\n+----------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"SlidingWindow\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\ndf_windowed = df.groupBy(window(\"timestamp\", \"10 minutes\")).count()\nquery = df_windowed.writeStream.outputMode(\"update\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:24:54.024738Z","iopub.execute_input":"2024-05-03T09:24:54.025129Z","iopub.status.idle":"2024-05-03T09:25:04.317554Z","shell.execute_reply.started":"2024-05-03T09:24:54.025102Z","shell.execute_reply":"2024-05-03T09:25:04.316259Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"24/05/03 09:24:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n24/05/03 09:24:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b4090ff9-0fe1-43ec-bb5c-d8b97806e7c6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 09:24:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-----+\n|window|count|\n+------+-----+\n+------+-----+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------\nBatch: 1\n-------------------------------------------\n+--------------------+-----+\n|              window|count|\n+--------------------+-----+\n|{2024-05-03 09:20...|    5|\n+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"JoinStreams\").getOrCreate()\ndf1 = spark.readStream.format(\"rate\").load()\ndf2 = spark.readStream.format(\"rate\").load()\ndf_joined = df1.join(df2, \"value\")\nquery = df_joined.writeStream.outputMode(\"append\").format(\"console\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:25:44.472944Z","iopub.execute_input":"2024-05-03T09:25:44.473360Z","iopub.status.idle":"2024-05-03T09:25:54.722976Z","shell.execute_reply.started":"2024-05-03T09:25:44.473331Z","shell.execute_reply":"2024-05-03T09:25:54.719962Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"24/05/03 09:25:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n24/05/03 09:25:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n24/05/03 09:25:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n24/05/03 09:25:54 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n24/05/03 09:25:54 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n24/05/03 09:25:54 WARN Shell: Interrupted while joining on: Thread[Thread-20457,5,main]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:679)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 WARN Shell: Interrupted while joining on: Thread[Thread-20458,5,]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:677)\n\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1356)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:185)\n\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:219)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:805)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:812)\n\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:319)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.init(HDFSBackedStateStoreProvider.scala:248)\n\tat org.apache.spark.sql.execution.streaming.state.StateStoreProvider$.createAndInit(StateStore.scala:330)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$6(StateStore.scala:549)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$5(StateStore.scala:548)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:546)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:506)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:417)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:441)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:385)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:529)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:276)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:241)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:241)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:295)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 WARN Shell: Interrupted while joining on: Thread[Thread-20459,5,main]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:342)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:679)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 WARN Shell: Interrupted while joining on: Thread[Thread-20463,5,]\njava.lang.InterruptedException\n\tat java.base/java.lang.Object.wait(Native Method)\n\tat java.base/java.lang.Thread.join(Thread.java:1300)\n\tat java.base/java.lang.Thread.join(Thread.java:1375)\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:677)\n\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1356)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:185)\n\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:219)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:805)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:812)\n\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:319)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.init(HDFSBackedStateStoreProvider.scala:248)\n\tat org.apache.spark.sql.execution.streaming.state.StateStoreProvider$.createAndInit(StateStore.scala:330)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$6(StateStore.scala:549)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$5(StateStore.scala:548)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:546)\n\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:506)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:417)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:441)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:385)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:529)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:276)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:241)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:241)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:295)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 ERROR Utils: Aborting task\njava.lang.IllegalStateException: Error committing version 1 into HDFSStateStore[id=(op=0,part=125),dir=file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/125/left-keyWithIndexToValue]\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:679)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: File file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/125/left-keyWithIndexToValue/..1.delta.3d62ec7b-e8a7-464b-b9ca-675a27bffb00.TID1550.tmp.crc does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\t... 33 more\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborting commit for partition 125 (task 1550, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborted commit for partition 125 (task 1550, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR TaskContextImpl: Error in TaskCompletionListener\njava.io.FileNotFoundException: File file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/125/right-keyToNumValues does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 WARN TaskSetManager: Lost task 125.0 in stage 74.0 (TID 1550) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 45 cancelled part of cancelled job group 30c53e03-24e0-4e82-9789-746a039302d5)\n24/05/03 09:25:54 ERROR Utils: Aborting task\norg.apache.spark.TaskKilledException\n\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborting commit for partition 127 (task 1552, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborted commit for partition 127 (task 1552, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR TaskContextImpl: Error in TaskCompletionListener\njava.io.FileNotFoundException: File file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/127/right-keyToNumValues does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 ERROR TaskContextImpl: Error in TaskCompletionListener\njava.io.FileNotFoundException: File file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/127/left-keyToNumValues does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 WARN TaskSetManager: Lost task 127.0 in stage 74.0 (TID 1552) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 45 cancelled part of cancelled job group 30c53e03-24e0-4e82-9789-746a039302d5)\n24/05/03 09:25:54 ERROR Utils: Aborting task\norg.apache.spark.TaskKilledException\n\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:224)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborting commit for partition 128 (task 1553, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborted commit for partition 128 (task 1553, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR Utils: Aborting task\njava.lang.IllegalStateException: Error committing version 1 into HDFSStateStore[id=(op=0,part=126),dir=file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/126/left-keyWithIndexToValue]\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:398)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:343)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:679)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$27(StreamingSymmetricHashJoinExec.scala:469)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:129)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:468)\n\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$29(StreamingSymmetricHashJoinExec.scala:488)\n\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: ExitCodeException exitCode=1: chmod: cannot access '/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/126/left-keyWithIndexToValue/..1.delta.387e270a-8835-46c2-a462-83c71be7129c.TID1551.tmp.crc': No such file or directory\n\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n\t... 33 more\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborting commit for partition 126 (task 1551, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR DataWritingSparkTask: Aborted commit for partition 126 (task 1551, attempt 0, stage 74.0)\n24/05/03 09:25:54 ERROR TaskContextImpl: Error in TaskCompletionListener\njava.io.FileNotFoundException: File file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/126/right-keyToNumValues does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:348)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 ERROR TaskContextImpl: Error in TaskCompletionListener\njava.io.FileNotFoundException: File file:/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0/126/left-keyWithIndexToValue does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:361)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:367)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:158)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.abortIfNeeded(SymmetricHashJoinStateManager.scala:405)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.abortIfNeeded(SymmetricHashJoinStateManager.scala:349)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.$anonfun$new$2$adapted(SymmetricHashJoinStateManager.scala:389)\n\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:137)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:199)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:144)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:137)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:177)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/03 09:25:54 WARN TaskSetManager: Lost task 126.0 in stage 74.0 (TID 1551) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 45 cancelled part of cancelled job group 30c53e03-24e0-4e82-9789-746a039302d5)\n24/05/03 09:25:54 WARN TaskSetManager: Lost task 128.0 in stage 74.0 (TID 1553) (7203d4b82bac executor driver): TaskKilled (Stage cancelled: Job 45 cancelled part of cancelled job group 30c53e03-24e0-4e82-9789-746a039302d5)\n24/05/03 09:25:54 WARN FileUtil: Failed to delete file or dir [/tmp/temporary-ccbaf3e6-9ba6-4885-a603-fc2532bf9368/state/0]: it still exists.\n","output_type":"stream"}]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"WriteToFile\").getOrCreate()\ndf = spark.readStream.format(\"rate\").load()\nquery = df.writeStream.format(\"parquet\").option(\"path\", \"output/\").start()\nquery.awaitTermination(10)\nquery.stop()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:27:19.563534Z","iopub.execute_input":"2024-05-03T09:27:19.564697Z","iopub.status.idle":"2024-05-03T09:27:20.484558Z","shell.execute_reply.started":"2024-05-03T09:27:19.564631Z","shell.execute_reply":"2024-05-03T09:27:20.481399Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"24/05/03 09:27:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriteToFile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m----> 3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m query\u001b[38;5;241m.\u001b[39mstop()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...)."],"ename":"AnalysisException","evalue":"checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...).","output_type":"error"}]}]}