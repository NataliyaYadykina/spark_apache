{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u9_i1TqpZGR1"
      },
      "outputs": [],
      "source": [
        "#Устанавливаем pySpark\n",
        "!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2MHgjKWVtqTs"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, avg, when, max, month\n",
        "from pyspark.sql.functions import countDistinct\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FTAY1pSZjVTy",
        "outputId": "6dffd7a9-1046-4046-84c0-b5a8fe50bca0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"order_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2630,\n        \"min\": 1036,\n        \"max\": 9980,\n        \"num_unique_values\": 99,\n        \"samples\": [\n          1638,\n          4656,\n          6252\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 276,\n        \"min\": 110,\n        \"max\": 995,\n        \"num_unique_values\": 96,\n        \"samples\": [\n          461,\n          247,\n          124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"customer_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26260,\n        \"min\": 11251,\n        \"max\": 99261,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          33611,\n          98041,\n          95568\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"order_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 86,\n        \"samples\": [\n          \"2024-02-16\",\n          \"2023-08-25\",\n          \"2023-09-26\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quantity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5,\n          4,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_per_unit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 291.90605633714256,\n        \"min\": 16.77,\n        \"max\": 993.13,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          822.21,\n          173.17,\n          297.66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2324.4349267625316,\n        \"min\": 25.69,\n        \"max\": 9905.9,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          5755.47,\n          346.34,\n          595.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"payment_method\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"cash\",\n          \"card\",\n          \"credit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"north\",\n          \"center\",\n          \"west\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1abaabbe-a9b4-4f8a-b482-b2a99515edd7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>order_date</th>\n",
              "      <th>quantity</th>\n",
              "      <th>price_per_unit</th>\n",
              "      <th>total_price</th>\n",
              "      <th>payment_method</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2818</td>\n",
              "      <td>128</td>\n",
              "      <td>11577</td>\n",
              "      <td>2023-08-25</td>\n",
              "      <td>1</td>\n",
              "      <td>796.66</td>\n",
              "      <td>796.66</td>\n",
              "      <td>cash</td>\n",
              "      <td>south</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2742</td>\n",
              "      <td>135</td>\n",
              "      <td>45498</td>\n",
              "      <td>2024-01-24</td>\n",
              "      <td>4</td>\n",
              "      <td>262.72</td>\n",
              "      <td>1050.88</td>\n",
              "      <td>card</td>\n",
              "      <td>north</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4155</td>\n",
              "      <td>328</td>\n",
              "      <td>62904</td>\n",
              "      <td>2023-11-21</td>\n",
              "      <td>4</td>\n",
              "      <td>871.24</td>\n",
              "      <td>3484.96</td>\n",
              "      <td>credit</td>\n",
              "      <td>west</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7955</td>\n",
              "      <td>795</td>\n",
              "      <td>82611</td>\n",
              "      <td>2023-04-26</td>\n",
              "      <td>3</td>\n",
              "      <td>477.74</td>\n",
              "      <td>1433.22</td>\n",
              "      <td>cash</td>\n",
              "      <td>ost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3169</td>\n",
              "      <td>378</td>\n",
              "      <td>74035</td>\n",
              "      <td>2024-03-18</td>\n",
              "      <td>6</td>\n",
              "      <td>875.12</td>\n",
              "      <td>5250.72</td>\n",
              "      <td>card</td>\n",
              "      <td>center</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1abaabbe-a9b4-4f8a-b482-b2a99515edd7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1abaabbe-a9b4-4f8a-b482-b2a99515edd7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1abaabbe-a9b4-4f8a-b482-b2a99515edd7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-870ac0a7-c67b-415a-b8fa-243462e80563\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-870ac0a7-c67b-415a-b8fa-243462e80563')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-870ac0a7-c67b-415a-b8fa-243462e80563 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   order_id  product_id  customer_id  order_date  quantity  price_per_unit  \\\n",
              "0      2818         128        11577  2023-08-25         1          796.66   \n",
              "1      2742         135        45498  2024-01-24         4          262.72   \n",
              "2      4155         328        62904  2023-11-21         4          871.24   \n",
              "3      7955         795        82611  2023-04-26         3          477.74   \n",
              "4      3169         378        74035  2024-03-18         6          875.12   \n",
              "\n",
              "   total_price payment_method  region  \n",
              "0       796.66           cash   south  \n",
              "1      1050.88           card   north  \n",
              "2      3484.96         credit    west  \n",
              "3      1433.22           cash     ost  \n",
              "4      5250.72           card  center  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('sales_data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdz-4j1Tji_Y",
        "outputId": "820c3aed-9b90-4796-d876-be1cdf8a3e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   order_id        100 non-null    int64  \n",
            " 1   product_id      100 non-null    int64  \n",
            " 2   customer_id     100 non-null    int64  \n",
            " 3   order_date      100 non-null    object \n",
            " 4   quantity        100 non-null    int64  \n",
            " 5   price_per_unit  100 non-null    float64\n",
            " 6   total_price     100 non-null    float64\n",
            " 7   payment_method  100 non-null    object \n",
            " 8   region          100 non-null    object \n",
            "dtypes: float64(2), int64(4), object(3)\n",
            "memory usage: 7.2+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naxU9VX0Y8xS",
        "outputId": "f8dbc45b-0cf6-4a59-b80d-b55566cf98ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+------------------+---------+\n",
            "|order_month|total_sales|     average_sales|max_sales|\n",
            "+-----------+-----------+------------------+---------+\n",
            "|          1|      23306|           2913.25|     6537|\n",
            "|          2|      12103|2017.1666666666667|     5625|\n",
            "|          3|      30389|          3798.625|     9905|\n",
            "|          4|      28471| 2190.076923076923|     7821|\n",
            "|          5|      20784|            3464.0|     8744|\n",
            "|          6|      23232|            2323.2|     5486|\n",
            "|          7|      15185|          1898.125|     5444|\n",
            "|          8|      30575|            3057.5|     8444|\n",
            "|          9|      26291|          3286.375|     8056|\n",
            "|         10|      25513|          3189.125|     6531|\n",
            "|         11|      21918|           2739.75|     6667|\n",
            "|         12|      13716|1959.4285714285713|     5755|\n",
            "+-----------+-----------+------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Создаем SparkSession\n",
        "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
        "\n",
        "#загружаем csv файл\n",
        "'''\n",
        "    spark.read.load: Это метод PySpark для чтения данных из файла. Он используется для загрузки данных в DataFrame.\n",
        "    'sales_data.csv': Это путь к файлу CSV, который вы хотите прочитать. В данном случае, файл называется sales_data.csv.\n",
        "    format=\"csv\": Указывает формат файла, который вы хотите прочитать. В данном случае, формат файла - CSV.\n",
        "    sep=\",\": Этот параметр определяет разделитель полей в CSV-файле. В данном случае, разделителем является запятая.\n",
        "    header=\"true\": Этот параметр указывает, что первая строка CSV-файла содержит имена столбцов.\n",
        "    Infer_schema=True: Этот параметр указывает PySpark автоматически определять типы данных столбцов на основе содержимого файла.\n",
        "'''\n",
        "df_pyspark=spark.read.load('sales_data.csv', format=\"csv\", sep=\",\", header=\"true\", Infer_schema=True)\n",
        "\n",
        "#необходимые преобразования\n",
        "'''\n",
        "Изменяем тип данных столбца `order_id` в DataFrame `df_pyspark` на целочисленный (`int`).\n",
        "Это достигается с помощью функции `withColumn()`, которая используется для добавления, замены или\n",
        "обновления столбцов в DataFrame. В данном случае, `withColumn()` применяется для изменения типа\n",
        "данных столбца `order_id` с использованием метода `cast()`, который принимает в качестве аргумента\n",
        "тип данных, на который нужно преобразовать столбец. В данном случае, тип данных для преобразования\n",
        "указан как `'int'`, что соответствует целочисленному типу данных в PySpark.\n",
        "\n",
        "Пример кода:\n",
        "```python\n",
        "df_pyspark = df_pyspark.withColumn(\"order_id\", df_pyspark[\"order_id\"].cast('int'))\n",
        "```\n",
        "Код создает новый DataFrame, в котором тип данных столбца `order_id` изменен на `int`, не\n",
        "изменяя исходный DataFrame `df_pyspark`. Функция `withColumn()` является трансформацией DataFrame,\n",
        "что означает, что она возвращает новый DataFrame с указанными изменениями, не изменяя исходный DataFrame.\n",
        "'''\n",
        "df_pyspark = df_pyspark.withColumn(\"order_id\", df_pyspark[\"order_id\"].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn(\"product_id\", df_pyspark[\"product_id\"].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn(\"customer_id\", df_pyspark[\"customer_id\"].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn(\"order_date\", df_pyspark[\"order_date\"].cast('date'))\n",
        "df_pyspark = df_pyspark.withColumn(\"quantity\", df_pyspark[\"quantity\"].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn(\"price_per_unit\", df_pyspark[\"price_per_unit\"].cast('int'))\n",
        "df_pyspark = df_pyspark.withColumn(\"total_price\", df_pyspark[\"total_price\"].cast('int'))\n",
        "\n",
        "#добавление колонки со значением месяца\n",
        "df_pyspark = df_pyspark.withColumn(\"order_month\", month(df_pyspark[\"order_date\"]))\n",
        "\n",
        "#нахождение нужных значений\n",
        "'''\n",
        "Выполняем агрегацию данных в DataFrame `df_pyspark` по столбцу `order_month`,\n",
        "используя функции `groupBy` и `agg` из PySpark. В результате получаем новый DataFrame\n",
        "`sales_analysis`, который содержит следующие агрегированные данные:\n",
        "\n",
        "- `total_sales`: сумма значений столбца `total_price` для каждого месяца.\n",
        "- `average_sales`: среднее значение столбца `total_price` для каждого месяца.\n",
        "- `max_sales`: максимальное значение столбца `total_price` для каждого месяца.\n",
        "\n",
        "Это позволяет анализировать продажи по месяцам, выявляя общую сумму продаж,\n",
        "среднюю сумму продаж и максимальную сумму продаж для каждого месяца.\n",
        "'''\n",
        "sales_analysis = df_pyspark.groupBy(\"order_month\").agg(\n",
        "                            sum(\"total_price\").alias(\"total_sales\"),\n",
        "                            avg(\"total_price\").alias(\"average_sales\"),\n",
        "                            max(\"total_price\").alias(\"max_sales\"))\n",
        "\n",
        "#вывод результата с сортировкой\n",
        "sales_analysis.orderBy('order_month').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07p2rV7ljs9H",
        "outputId": "a394809d-5a89-4b3d-ccfc-cf3ebfee2d96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StructType([StructField('order_id', IntegerType(), True), StructField('product_id', IntegerType(), True), StructField('customer_id', IntegerType(), True), StructField('order_date', DateType(), True), StructField('quantity', IntegerType(), True), StructField('price_per_unit', IntegerType(), True), StructField('total_price', IntegerType(), True), StructField('payment_method', StringType(), True), StructField('region', StringType(), True), StructField('order_month', IntegerType(), True)])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pyspark.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-lirKpeZT1g"
      },
      "source": [
        "### Задание 2: Вычислите количество товаров, купленных различными методами оплаты.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syxKtruti038"
      },
      "source": [
        "Код выполняет агрегацию данных в DataFrame `df_pyspark` по столбцу `payment_method`, суммируя значения в столбце `quantity` для каждой группы и присваивая результату новое имя `num_of_sales`. В результате получается новый DataFrame `sales_analysis`, в котором каждая строка соответствует уникальному значению из столбца `payment_method`, а столбец\n",
        "`num_of_sales` содержит сумму значений `quantity` для каждой группы.\n",
        "\n",
        "- `groupBy(\"payment_method\")` группирует данные по столбцу `payment_method`, создавая группы для каждого уникального значения в этом столбце.\n",
        "- `agg(sum(\"quantity\").alias(\"num_of_sales\"))` применяет агрегационную функцию `sum` к столбцу `quantity` для каждой группы, суммируя значения `quantity` внутри каждой группы. Результат этой операции затем переименовывается в `num_of_sales` с помощью метода `alias`.\n",
        "\n",
        "Таким образом, код позволяет анализировать продажи по различным методам оплаты, суммируя количество продаж для каждого метода оплаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyOOjaMWZWSf",
        "outputId": "f240a800-02d0-4be0-c662-e61eecc54822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------------+\n",
            "|payment_method|num_of_sales|\n",
            "+--------------+------------+\n",
            "|          cash|         152|\n",
            "|          card|         182|\n",
            "|        credit|         185|\n",
            "+--------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#нахождение нужных значениях\n",
        "sales_analysis = df_pyspark.groupBy(\"payment_method\").agg(sum(\"quantity\").alias(\"num_of_sales\"))\n",
        "\n",
        "#вывод результата\n",
        "sales_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN9WbJQiueVo"
      },
      "source": [
        "----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ddxISE3Zcj9"
      },
      "source": [
        "### Задание 3: Найдите регион с самым большой суммарной стоимостью продаж\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBUx9TzgfL_x"
      },
      "source": [
        "1. Группируем данные в DataFrame `df_pyspark` по столбцу `region`, а затем применяет агрегационную функцию `sum` к столбцу `total_price` для каждой группы. Результат этой операции переименовывается в `total_price_per_region`.\n",
        "2. Создаем новый DataFrame `sales_analysis`, содержащий сумму `total_price` для каждой уникальной `region`.\n",
        "3. Выбираем столбец `region` из DataFrame `sales_analysis`, сортирует его по убыванию значений `total_price_per_region` и выбирает первую запись.\n",
        "4. Выводим результат, который представляет собой регион с наибольшей суммой `total_price`.\n",
        "\n",
        "Таким образом, код анализирует продажи по регионам, суммирует общую стоимость продаж для каждого региона и определяет регион с наибольшей суммой продаж."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SzQa3f-Zd6P",
        "outputId": "5a88b648-acbf-4d19-e59c-2d12de8b5d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "west\n",
            "+------+----------------------+\n",
            "|region|total_price_per_region|\n",
            "+------+----------------------+\n",
            "|  west|                 81691|\n",
            "|   ost|                 59872|\n",
            "|center|                 47258|\n",
            "| north|                 30214|\n",
            "| south|                 52448|\n",
            "+------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#нахождение нужных значениях\n",
        "sales_analysis = df_pyspark.groupBy(\"region\").agg(\n",
        "    sum(\"total_price\").alias(\"total_price_per_region\")\n",
        ")\n",
        "total_price_per_region = sales_analysis.select(\"region\").orderBy(sales_analysis.total_price_per_region.desc()).first()\n",
        "\n",
        "#вывод результата\n",
        "print(*total_price_per_region)\n",
        "sales_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoxgAb7vumEL"
      },
      "source": [
        "--------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5VwaTqpZieZ"
      },
      "source": [
        "### Задание 4: Вычислите общую сумму продаж и среднюю сумму продажи для каждого региона.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGXeHTG5fU5X"
      },
      "source": [
        "1. Группируем данные в DataFrame `df_pyspark` по столбцу `region`, а затем применяет агрегационные функции `sum` и `avg` к столбцу `total_price` для каждой группы. Результаты этих операций переименовываются в `total_price_per_region` и `avg_price_per_region` соответственно.\n",
        "2. Создаем новый DataFrame `sales_analysis`, содержащий сумму `total_price` и среднее значение `total_price` для каждой уникальной `region`.\n",
        "3. Выводим результаты из DataFrame `sales_analysis` с помощью метода `show()`, который отображает содержимое DataFrame в консоли.\n",
        "\n",
        "Таким образом, код анализирует продажи по регионам, суммирует общую стоимость продаж и вычисляет среднюю стоимость продаж для каждого региона. Это позволяет получить общую картину продаж по регионам, включая как общую стоимость продаж, так и среднюю стоимость продаж в каждом регионе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAR3SCC_ZirD",
        "outputId": "5ea45c7a-6a49-46d5-ff3e-7cd509fb94d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------------------+--------------------+\n",
            "|region|total_price_per_region|avg_price_per_region|\n",
            "+------+----------------------+--------------------+\n",
            "|  west|                 81691|             4084.55|\n",
            "|   ost|                 59872|              2993.6|\n",
            "|center|                 47258|              2362.9|\n",
            "| north|                 30214|              1510.7|\n",
            "| south|                 52448|              2622.4|\n",
            "+------+----------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#нахождение нужных значениях\n",
        "sales_analysis = df_pyspark.groupBy(\"region\").agg(\n",
        "    sum(\"total_price\").alias(\"total_price_per_region\"),\n",
        "    avg(\"total_price\").alias(\"avg_price_per_region\")\n",
        ")\n",
        "#вывод результата\n",
        "sales_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO37aDRFurog"
      },
      "source": [
        "-------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYUYfDqbZoUl"
      },
      "source": [
        "### Задание 5: Вычислите общее количество и сумму товаров, проданных за наличные в 2023 году.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S5XRpapfhDb"
      },
      "source": [
        "1. Добавляем новую колонку `order_year` в DataFrame `df_pyspark`, используя функцию `year` для извлечения года из даты в колонке `order_date`.\n",
        "2. Фильтруем DataFrame `df_pyspark` по двум условиям: методу оплаты должен быть \"Наличные\" и год заказа должен быть 2023.\n",
        "3. Агрегируем данные, суммируя количество (`quantity`) и общую стоимость (`total_price`) для всех записей, удовлетворяющих условиям фильтрации. Результаты этих операций переименовываются в `total_quantity_2023` и `total_price_2023` соответственно.\n",
        "4. Создаем новый DataFrame `sales_analysis`, содержащий сумму количества и общую стоимость продаж за 2023 год, оплаченных наличными.\n",
        "5. Выводим результаты из DataFrame `sales_analysis` с помощью метода `show()`, который отображает содержимое DataFrame в консоли.\n",
        "\n",
        "Таким образом, код анализирует продажи, оплаченные наличными, за 2023 год, суммируя количество и общую стоимость продаж для этого периода."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1-mw38JZqmJ",
        "outputId": "8833e357-96ee-448b-e838-6d53dc06536b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----------------+\n",
            "|total_quantity_2023|total_price_2023|\n",
            "+-------------------+----------------+\n",
            "|                121|           71739|\n",
            "+-------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#добавление колонки со значением года\n",
        "df_pyspark = df_pyspark.withColumn(\"order_year\", F.year(df_pyspark[\"order_date\"]))\n",
        "\n",
        "#нахождение нужных значений\n",
        "sales_analysis = df_pyspark.filter((df_pyspark.payment_method == \"cash\") & (df_pyspark.order_year == 2023)).agg(\n",
        "    sum(\"quantity\").alias(\"total_quantity_2023\"),\n",
        "    sum(\"total_price\").alias(\"total_price_2023\")\n",
        ")\n",
        "\n",
        "#вывод результата с сортировкой\n",
        "sales_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W43HjvgJZvDg"
      },
      "source": [
        "### Задание 6: Найдите уникальное количество покупателей за 2023 год\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGF3TaRrfp3x"
      },
      "source": [
        "1. Импортируем функцию `countDistinct` из модуля `pyspark.sql.functions`. Эта функция используется для подсчета уникальных значений в определенных столбцах DataFrame.\n",
        "2. Фильтруем исходный DataFrame `df_pyspark`, оставляя только записи за 2023 год. Это делается с помощью метода `filter`, который принимает условие для фильтрации.\n",
        "3. Выбираем из отфильтрованного DataFrame количество уникальных значений в столбце `customer_id` с использованием функции `countDistinct`. Результат сохраняется в новом столбце с именем `unique_users` [1].\n",
        "4. Выводим результат на экран с помощью метода `show`. Этот метод отображает данные DataFrame в удобном для чтения формате.\n",
        "\n",
        "В итоге, код подсчитывает количество уникальных клиентов (`customer_id`), совершивших покупки в 2023 году, и выводит это количество на экран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z8hXbOIZwxB",
        "outputId": "a8f8dc91-f31a-49d3-be9d-fc98baffb1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|unique_users|\n",
            "+------------+\n",
            "|          78|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#нахождение нужных значений\n",
        "sales_analysis = df_pyspark.filter(df_pyspark.order_year == 2023)\n",
        "sales_analysis = sales_analysis.select(countDistinct(\"customer_id\").alias(\"unique_users\"))\n",
        "\n",
        "#вывод результата с сортировкой\n",
        "sales_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMMf7s8_Z0u5"
      },
      "source": [
        "### Задание 7: Вам даны данные с информацией о стоимости продуктов в различных валютах. Ваша задача состоит в том, чтобы перевести все цены в доллары, используя текущие курсы валют. Однако у вас есть ограничение: для некоторых продуктов курс валюты неизвестен и их стоимость должна остаться в исходной валюте. (Для конвертации из EUR в USD нужно умножить на 1.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09iuWD2Kf0Nk"
      },
      "source": [
        "1. Импортируем необходимые модули и функции из PySpark для работы с SparkSession и функцией `when` для условной логики.\n",
        "2. Создаем сессию Spark с именем приложения \"currency_conversion\". Это необходимо для работы с Spark и выполнения операций с данными.\n",
        "3. Загружаем данные в DataFrame `df` с помощью метода `createDataFrame`. Данные представляют собой список кортежей, где каждый кортеж содержит информацию о продукте: его идентификатор (`product_id`), цену (`price`) и валюту (`currency`).\n",
        "4. Добавляем новый столбец `price_usd` в DataFrame, который содержит цену продукта в долларах США. Для этого используется функция `when` для условного преобразования цены в зависимости от валюты:\n",
        "   - Если валюта равна \"USD\", цена остается без изменений.\n",
        "   - Если валюта равна \"EUR\", цена умножается на курс обмена 1.2 (примерный курс EUR/USD).\n",
        "   - Если валюта неизвестна, цена остается без изменений.\n",
        "5. Выводим результат на экран с помощью метода `show`. Это позволяет увидеть, как изменились цены после конвертации в доллары США.\n",
        "\n",
        "В итоге, код позволяет преобразовать цены продуктов из разных валют в доллары США, используя примерный курс обмена для EUR/USD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa6-W5fOZ2h7",
        "outputId": "a62b80fd-bfa3-425f-e910-f4347e0028b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+--------+---------+\n",
            "|product_id|price|currency|price_usd|\n",
            "+----------+-----+--------+---------+\n",
            "|         1|  100|     USD|    100.0|\n",
            "|         2|  200|     EUR|    240.0|\n",
            "|         3|  300| Unknown|    300.0|\n",
            "|         4|  100|     EUR|    120.0|\n",
            "|         5|  200|     EUR|    240.0|\n",
            "|         6|  300| Unknown|    300.0|\n",
            "|         7|  100| Unknown|    100.0|\n",
            "|         8|  200|     USD|    200.0|\n",
            "|         9|  300|     USD|    300.0|\n",
            "+----------+-----+--------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Создание сессии Spark\n",
        "spark = SparkSession.builder.appName(\"currency_conversion\").getOrCreate()\n",
        "\n",
        "# Загрузка датасета\n",
        "data = [(1, 100, \"USD\"),\n",
        "        (2, 200, \"EUR\"),\n",
        "        (3, 300, \"Unknown\"),\n",
        "        (4, 100, \"EUR\"),\n",
        "        (5, 200, \"EUR\"),\n",
        "        (6, 300, \"Unknown\"),\n",
        "        (7, 100, \"Unknown\"),\n",
        "        (8, 200, \"USD\"),\n",
        "        (9, 300, \"USD\")\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"price\", \"currency\"])\n",
        "\n",
        "# Добавление столбца с конвертированной ценой\n",
        "df = df.withColumn(\"price_usd\", when(df.currency == \"USD\", df.price)\n",
        "                                 .when(df.currency == \"EUR\", df.price * 1.2)  # Примерный курс EUR/USD = 1.2\n",
        "                             \t.otherwise(df.price))  # Если курс неизвестен, оставляем в исходной валюте\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBdDk6HgZ8WL"
      },
      "source": [
        "### Задание 8: Допустим, есть два датасета: один содержит информацию о пользователях (user_id, name, age), а другой содержит информацию о покупках пользователей (user_id, product_id, date). Необходимо найти средний возраст пользователей, совершивших покупки.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXBhoxLQgJn9"
      },
      "source": [
        "1. Инициализируем Spark сессию с именем приложения \"user_purchase_join\".\n",
        "2. Создаем два DataFrame: `users_df` и `purchases_df`. Первый содержит информацию о пользователях (ID, имя, возраст), второй - о покупках (ID пользователя, ID продукта, дата покупки).\n",
        "3. Выполняем операцию соединения (join) между этими двумя DataFrame по столбцу `user_id`. Это позволяет объединить информацию о пользователях и их покупках в одном DataFrame.\n",
        "4. Группируем результат по `user_id` и вычисляет средний возраст пользователей, которые совершили покупки.\n",
        "5. Выводим результат на экран.\n",
        "\n",
        "Основная цель кода - анализировать данные о покупках пользователей, вычисляя средний возраст пользователей, совершивших покупки. Это может быть полезно для анализа поведения пользователей и оптимизации маркетинговых стратегий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jektE8ZlZ8KH",
        "outputId": "ae163748-a47b-46ee-a951-9d0330dc9a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+\n",
            "|user_id|average_age|\n",
            "+-------+-----------+\n",
            "|      1|       25.0|\n",
            "|      2|       30.0|\n",
            "|      3|       28.0|\n",
            "|      6|       17.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Создаем Spark сессию\n",
        "spark = SparkSession.builder.appName(\"user_purchase_join\").getOrCreate()\n",
        "\n",
        "# Загружаем датасеты\n",
        "users_df = spark.createDataFrame([\n",
        "\t(1, \"Alice\", 25),\n",
        "\t(2, \"Bob\", 30),\n",
        "\t(3, \"Charlie\", 28),\n",
        "\t(4, \"John\", 56),\n",
        "\t(5, \"Alex\", 41),\n",
        "\t(6, \"Juliya\", 17)\n",
        "\n",
        "], [\"user_id\", \"name\", \"age\"])\n",
        "\n",
        "purchases_df = spark.createDataFrame([\n",
        "\t(1, 101, \"2022-01-01\"),\n",
        "\t(2, 102, \"2022-01-02\"),\n",
        "\t(3, 103, \"2022-01-03\"),\n",
        "\t(3, 104, \"2022-01-04\"),\n",
        "\t(6, 105, \"2022-01-05\")\n",
        "], [\"user_id\", \"product_id\", \"date\"])\n",
        "\n",
        "# Производим операцию join и вычисляем средний возраст\n",
        "result_df = users_df.join(purchases_df, \"user_id\").groupBy(\"user_id\").agg(avg(\"age\").alias(\"average_age\"))\n",
        "\n",
        "# Выводим результат\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl7gn6SlaECs"
      },
      "source": [
        "### Задание 9: У вас есть два набора данных. Первый набор содержит информацию о продуктах: id продукта, название, категория и цена. Второй набор содержит информацию о заказах: id заказа, id продукта, количество. Ваша задача — использовать PySpark для выполнения следующих шагов:\n",
        "\n",
        "1. Присоединить набор данных о продуктах к набору данных о заказах с помощью id продукта.\n",
        "3. Рассчитать общую стоимость каждого заказа, учитывая количество продуктов и их цену.\n",
        "4. Отфильтровать заказы, у которых общая стоимость больше 1000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEDkVCHzgRvq"
      },
      "source": [
        "1. Инициализируем сессию Spark с именем приложения \"aggregate-join-filter\".\n",
        "\n",
        "2. Создаем два DataFrame (`products_df` и `orders_df`) из предопределенных списков кортежей, представляющих данные о продуктах и заказах соответственно.\n",
        "\n",
        "3. Присоединяем DataFrame `orders_df` к DataFrame `products_df` по столбцу \"product_id\", используя внутреннее соединение (inner join) по умолчанию.\n",
        "\n",
        "4. Добавляем новый столбец \"total_cost\" в DataFrame `joined_df`, вычисляя общую стоимость каждого заказа как произведение количества товара на цену товара.\n",
        "\n",
        "5. Отфильтровываем заказы, у которых общая стоимость больше 1000, и сохраняет результат в DataFrame `filtered_orders_df`.\n",
        "\n",
        "6. Выводим результаты фильтрации на экран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTdFqARAaMHl",
        "outputId": "c921eb72-8686-43cf-de8b-c61ff3093d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------+--------+--------+---------+-----+----------+\n",
            "|product_id|order_id|quantity|   title| category|price|total_cost|\n",
            "+----------+--------+--------+--------+---------+-----+----------+\n",
            "|         4|       5|       4|product4|category3| 20.0|      80.0|\n",
            "|         6|       7|       3|product6|category3| 25.0|      75.0|\n",
            "|         9|      10|       4|product9|category3| 22.0|      88.0|\n",
            "+----------+--------+--------+--------+---------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Создание сессии Spark\n",
        "spark = SparkSession.builder.appName(\"aggregate-join-filter\").getOrCreate()\n",
        "\n",
        "# Чтение наборов данных из CSV файлов\n",
        "data_produсts = [(1, \"product1\", \"category1\", 10.0),\n",
        "                (2, \"product2\", \"category2\", 15.0),\n",
        "                (3, \"product3\", \"category1\", 12.5),\n",
        "                (4, \"product4\", \"category3\", 20.0),\n",
        "                (5, \"product5\", \"category2\", 18.0),\n",
        "                (6, \"product6\", \"category3\", 25.0),\n",
        "                (7, \"product7\", \"category1\", 9.0),\n",
        "                (8, \"product8\", \"category2\", 16.0),\n",
        "                (9, \"product9\", \"category3\", 22.0),\n",
        "                (10, \"product10\", \"category1\", 11.5)]\n",
        "\n",
        "products_df = spark.createDataFrame(data_produсts, [\"product_id\", \"title\", \"category\", \"price\"])\n",
        "\n",
        "data_orders = [(1, 1, 5),\n",
        "               (2, 3, 2),\n",
        "               (3, 2, 3),\n",
        "               (4, 5, 1),\n",
        "               (5, 4, 4),\n",
        "               (6, 7, 2),\n",
        "               (7, 6, 3),\n",
        "               (8, 8, 2),\n",
        "               (9, 10, 1),\n",
        "               (10, 9, 4)]\n",
        "\n",
        "orders_df = spark.createDataFrame(data_orders, [\"order_id\", \"product_id\", \"quantity\"])\n",
        "\n",
        "# Присоединение набора данных о продуктах к набору данных о заказах\n",
        "joined_df = orders_df.join(products_df, \"product_id\")\n",
        "\n",
        "# Расчет общей стоимости каждого заказа\n",
        "total_cost_df = joined_df.withColumn(\"total_cost\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Отфильтрование заказов с общей стоимостью больше 50\n",
        "filtered_orders_df = total_cost_df.filter(total_cost_df.total_cost > 50)\n",
        "\n",
        "# Вывод результата\n",
        "filtered_orders_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzvtUy16aTR2"
      },
      "source": [
        "### Задание 10: Найти сумму чисел в колонке\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opFKR8vfgck2"
      },
      "source": [
        "1. Создаем сессию Spark с именем приложения \"sum_example\".\n",
        "2. Создаем DataFrame с одним столбцом \"number\", содержащим числа от 1 до 4.\n",
        "3. Вычисляем сумму всех чисел в столбце \"number\" и выводит результат.\n",
        "\n",
        "Код использует функцию `sum` из модуля `pyspark.sql.functions`, которая является агрегатной функцией и возвращает сумму всех значений в указанном столбце. В данном случае, она применяется к столбцу \"number\" DataFrame `df` для вычисления общей суммы чисел в этом столбце. Результат вычисления суммы затем отображается с помощью метода `show()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FSAMl7zaVh8",
        "outputId": "f139fb1f-9a54-4f3c-b611-d919c9816be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|sum(number)|\n",
            "+-----------+\n",
            "|         10|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Создание Spark сессии\n",
        "spark = SparkSession.builder.appName(\"sum_example\").getOrCreate()\n",
        "\n",
        "# Создание DataFrame\n",
        "data = [(1,), (2,), (3,), (4,)]\n",
        "df = spark.createDataFrame(data, [\"number\"])\n",
        "\n",
        "# Найти сумму чисел в колонке \"number\"\n",
        "sum_result = df.select(sum(\"number\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIr0GdK0aZai"
      },
      "source": [
        "### Задание 11: Посчитать количество уникальных значений в колонке\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3APjuShgkTe"
      },
      "source": [
        "1. Импортируем необходимые модули из библиотеки PySpark для работы с Spark и функцией `countDistinct` для подсчета уникальных значений в DataFrame.\n",
        "2. Создаем экземпляр SparkSession с именем приложения \"count_distinct_example\". Это необходимо для инициализации Spark и подготовки среды для выполнения операций с данными.\n",
        "3. Создаем DataFrame с именем `df`, используя предоставленные данные. В данном случае, DataFrame содержит одну колонку \"name\" с именами \"Alice\", \"Bob\", \"Alice\" и \"Eve\".\n",
        "4. Выполняем подсчет количества уникальных значений в колонке \"name\" с помощью функции `countDistinct` и выводит результат на экран с помощью метода `show()`.\n",
        "\n",
        "В результате выполнения кода будет выведено количество уникальных имен в колонке \"name\", что в данном случае равно 3, так как имена \"Alice\" и \"Bob\" встречаются дважды, а \"Eve\" - один раз."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tThczq14aXrt",
        "outputId": "f285a814-a87c-42cf-f22b-b4620dc8fcb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|count(DISTINCT name)|\n",
            "+--------------------+\n",
            "|                   3|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Создание Spark сессии\n",
        "spark = SparkSession.builder.appName(\"count_distinct_example\").getOrCreate()\n",
        "\n",
        "# Создание DataFrame\n",
        "data = [(\"Alice\",), (\"Bob\",), (\"Alice\",), (\"Eve\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "# Посчитать количество уникальных значений в колонке \"name\"\n",
        "count_result = df.select(countDistinct(\"name\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RVVb3Fmafx5"
      },
      "source": [
        "### Задание 12: Выполнить фильтрацию данных по определенному условию\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq9d0WFdgrqj"
      },
      "source": [
        "Данный код выполняет следующие действия:\n",
        "\n",
        "1. Импортируем класс `SparkSession` из модуля `pyspark.sql`, который необходим для создания сессии Spark.\n",
        "2. Создаем сессию Spark с именем приложения \"filter_example\".\n",
        "3. Создаем список кортежей `data`, содержащий пары имя-возраст.\n",
        "4. Создаем DataFrame `df` из списка `data`, указывая имена столбцов как \"name\" и \"age\".\n",
        "5. Фильтруем данные в DataFrame `df`, оставляя только те строки, где возраст меньше 30, и выводит результат на экран с помощью метода `show()`.\n",
        "\n",
        "Важно отметить, что метод `filter()` используется для фильтрации строк в DataFrame на основе заданного условия. В данном случае, условием является `df.age < 30`, что означает выборку строк, где значение в столбце \"age\" меньше 30. Результатом выполнения этого кода будет отображение на экране строк DataFrame, соответствующих этому условию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SC9wv6nah3i",
        "outputId": "0774ad3c-c092-45f4-cad3-f0da7f6fa2d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Eve| 20|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        " # Создание Spark сессии\n",
        "spark = SparkSession.builder.appName(\"filter_example\").getOrCreate()\n",
        "\n",
        "# Создание DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Eve\", 20), (\"Charlie\", 35)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Выполнить фильтрацию данных по возрасту младше 30\n",
        "filtered_data = df.filter(df.age < 30).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OAOCIkSamLp"
      },
      "source": [
        "### Домашнее задание\n",
        "\n",
        "### Условие: дана таблица с колонками (id, name, salary, managerId), студентам необходимо написать код на spark, который создаст эту таблицу (данные указаны ниже) и в результате выдаст таблицу в которой будут имена сотрудников, которые зарабатывают больше своих менеджеров.\n",
        "\n",
        "Данные для таблицы:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "jU_wRNHbappb",
        "outputId": "0fa59e60-1475-402f-8566-bfc031b0c1eb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Henry\",\n          \"Max\",\n          \"Joe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Salary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"80\",\n          \"90\",\n          \"70\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ManagerId\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"3\",\n          \"4\",\n          \"Null\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ed733134-be95-4000-93c3-c81d1e94fa43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Salary</th>\n",
              "      <th>ManagerId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Joe</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Henry</td>\n",
              "      <td>80</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Sam</td>\n",
              "      <td>60</td>\n",
              "      <td>Null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Max</td>\n",
              "      <td>90</td>\n",
              "      <td>Null</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed733134-be95-4000-93c3-c81d1e94fa43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed733134-be95-4000-93c3-c81d1e94fa43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed733134-be95-4000-93c3-c81d1e94fa43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2bbbd924-e208-4b8d-872e-12c7ebcae9db\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2bbbd924-e208-4b8d-872e-12c7ebcae9db')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2bbbd924-e208-4b8d-872e-12c7ebcae9db button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   id   Name Salary ManagerId\n",
              "0   1    Joe     70         3\n",
              "1   2  Henry     80         4\n",
              "2   3    Sam     60      Null\n",
              "3   4    Max     90      Null"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = {'id': [1, 2, 3, 4], 'Name': ['Joe', 'Henry', 'Sam', 'Max'],\n",
        "        'Salary':['70', '80', '60', '90'], 'ManagerId':['3', '4', 'Null', 'Null']}\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI8uR6Xpb7XI"
      },
      "source": [
        "Результат должен быть:\n",
        "Joe (табличка с одной строкой и одним столбцом со значением Joe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8ZkW6pIOx4",
        "outputId": "2a1c5d15-9575-44b7-d103-bd3339c55944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+-------------+\n",
            "|Group|Value|MovingAverage|\n",
            "+-----+-----+-------------+\n",
            "|    A|    1|          1.0|\n",
            "|    A|    2|          1.5|\n",
            "|    A|    3|          2.0|\n",
            "|    B|    4|          4.0|\n",
            "|    B|    5|          4.5|\n",
            "|    B|    6|          5.0|\n",
            "+-----+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WindowFunctionExample\").getOrCreate()\n",
        "\n",
        "# Создание DataFrame\n",
        "data = [(\"A\", 1), (\"A\", 2), (\"A\", 3), (\"B\", 4), (\"B\", 5), (\"B\", 6)]\n",
        "df = spark.createDataFrame(data, [\"Group\", \"Value\"])\n",
        "\n",
        "# Определение окна для каждой группы\n",
        "windowSpec = Window.partitionBy(\"Group\").orderBy(\"Value\")\n",
        "\n",
        "# Вычисление скользящего среднего\n",
        "df = df.withColumn(\"MovingAverage\", avg(\"Value\").over(windowSpec.rowsBetween(-2, 0)))\n",
        "\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj84Ic1fI1Sz"
      },
      "outputs": [],
      "source": [
        "# 1/ Ранжирующие\n",
        "# --------------\n",
        "# row_number - присваивает уникальный номер каждой строке в окне\n",
        "# rank - присваивает уникальный номер набору строк в окне с пропусками ранга для последующих строк\n",
        "# percent_rank - вычисляет аналитический ранг в строке\n",
        "# dense_rank - присваивает уникальный номер набору строк в окне без пропусков ранга для последующих строк\n",
        "# ntile - присваивает ранг каждой строке в окне, но с возможностью пропуска значения ранга\n",
        "\n",
        "# 2/ Аналитические\n",
        "# ----------------\n",
        "# lag - возвращает значение строки, находящейся перед текущей строкой, сдвиг окна назад\n",
        "# lead - возвращает значение строки, находящейся после текущей строки, сдвиг окна вперед\n",
        "# cume_dist - вычисление кумулятивного значения распределения в окне\n",
        "# nth_value - возвращает значение строки, которая находится на n-ном месте в окне\n",
        "\n",
        "# 3/ Аггрегирущие\n",
        "# ---------------\n",
        "# min - \n",
        "# max - \n",
        "# count - \n",
        "# std - \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lMihD1mL9uo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import cume_dist\n",
        "\n",
        "# Определение спецификации окна\n",
        "windowSpec = Window.partitionBy(\"course\").orderBy(\"income\")\n",
        "\n",
        "# Применение оконной функции cume_dist к DataFrame\n",
        "df1.withColumn(\"cume_dist\", cume_dist().over(windowSpec)).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJAtts-JMRc7"
      },
      "outputs": [],
      "source": [
        "# 1. PartitionBy()\n",
        "# 2. OrderBy()\n",
        "# 3. rowsBetweens(start, end)\n",
        "# 4. rangeBetwen(start, end)\n",
        "\n",
        "# Window.unboudedPreceding, Window.unboudedFolliwing, Window.currentRow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31PtOnIKNKT1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Определение окна с разделением на партиции по столбцу \"country\" и сортировкой по столбцу \"date\"\n",
        "window = Window.partitionBy(\"country\").orderBy(\"date\")\n",
        "\n",
        "# Определение окна с границами, охватывающими строки от начала до текущей строки\n",
        "window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Определение окна с границами, охватывающими строки от 3 строк до текущей строки\n",
        "window = Window.orderBy(\"date\").rowsBetween(-3, Window.currentRow)\n",
        "\n",
        "# Определение окна с границами, охватывающими строки от 3 строк до 3 строк после текущей строки\n",
        "window = Window.orderBy(\"date\").rowsBetween(-3, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMIGEMt-N0c8"
      },
      "outputs": [],
      "source": [
        "1. Гибкость в анализе данных\n",
        "2. Сохранение идентичности иходнызх данных\n",
        "3. Поддержка сложных аналитических запросов\n",
        "4. Оптимизация по производительности\n",
        "5. Поддержка различых типов окон"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiWlO2DtPkOR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Определение окна с разделением на партиции по столбцу \"department\" и сортировкой по столбцу \"salary\"\n",
        "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "# Применение оконной функции avg с использованием rowsBetween\n",
        "df.withColumn(\"avg_salary\", avg(\"salary\").over(windowSpec.rowsBetween(Window.unboundedPreceding, Window.currentRow)))\n",
        "\n",
        "# Применение оконной функции avg с использованием rangeBetween\n",
        "df.withColumn(\"avg_salary_range\", avg(\"salary\").over(windowSpec.rangeBetween(Window.unboundedPreceding, 1000)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7YUA0gMQHJ5"
      },
      "outputs": [],
      "source": [
        "rowsBetween(Window.currentRow, 1)\n",
        "rangeBetween(Window.currentRow, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pleLLuQBQVl9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as func\n",
        "\n",
        "# Определение окна с использованием rowsBetween\n",
        "window1 = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\n",
        "\n",
        "# Определение окна с использованием rangeBetween\n",
        "window2 = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
        "\n",
        "# Применение оконных функций к DataFrame\n",
        "df.withColumn(\"sum_rows\", func.sum(\"id\").over(window1)) \\\n",
        " .withColumn(\"sum_range\", func.sum(\"id\").over(window2)) \\\n",
        " .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N5fselnRpms"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCixNnaAQm7t"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Определение пользовательской функции\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "# Преобразование функции в UDF\n",
        "add_one_udf = udf(add_one, IntegerType())\n",
        "\n",
        "# Использование UDF в DataFrame\n",
        "df = pyspark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\n",
        "df.withColumn(\"value_plus_one\", add_one_udf(df[\"value\"])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lMKrioXRNcj"
      },
      "outputs": [],
      "source": [
        "1. IntergerType\n",
        "2. DoubleType\n",
        "3. StringType\n",
        "4. BooleanType\n",
        "5. ArrayType\n",
        "6. MapType\n",
        "7. StructType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPe5JIquSOMt"
      },
      "outputs": [],
      "source": [
        "1. udf\n",
        "2. registrUDF\n",
        "3. unregistUDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1JhaDHbSlO4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Определение пользовательской функции\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "# Преобразование функции в UDF\n",
        "add_one_udf = udf(add_one, IntegerType())\n",
        "\n",
        "# Использование UDF в DataFrame\n",
        "df = spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\n",
        "df.withColumn(\"value_plus_one\", add_one_udf(df[\"value\"])).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
