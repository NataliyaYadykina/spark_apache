{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qJ6M2Kwn-T9r"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC5R-mCo-oFe"
      },
      "source": [
        "# Задача 1\n",
        "\n",
        "У Вас есть RDD со следующей структуройЖ (ключ, значение). Напишите программу на Apache Spark, которая найдет среднее значение для каждого уникального ключа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKBhE1La-p6c",
        "outputId": "93678452-aae4-4a62-fbfc-288a772a325b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key1: 30.0\n",
            "key2: 30.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Average By Key Example\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "dataRDD = sc.parallelize([\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 20),\n",
        "    (\"key1\", 30),\n",
        "    (\"key2\", 40),\n",
        "    (\"key1\", 50)\n",
        "])\n",
        "\n",
        "avarageRDD = dataRDD \\\n",
        "   .mapValues(lambda value: (value, 1)) \\\n",
        "   .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
        "   .mapValues(lambda sum_count: sum_count[0] / sum_count[1])\n",
        "\n",
        "# Collect the data and print it\n",
        "result = avarageRDD.collect()\n",
        "\n",
        "for key, value in result:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqlTSG8c-zKF"
      },
      "source": [
        "# Задача 2\n",
        "\n",
        "У Вас есть RDD со следующей структуройЖ (ключ, значение). Напишите программу на Apache Spark, которая найдет максимальное значение для каждого уникального ключа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAdF2cHr-z81",
        "outputId": "d1b03c52-28fa-4827-d828-cced04d386fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key1: 20\n",
            "key2: 15\n",
            "key3: 8\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MaxValueByKeyExample\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "data = ([\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 5),\n",
        "    (\"key1\", 20),\n",
        "    (\"key2\", 15),\n",
        "    (\"key3\", 8)\n",
        "])\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "max_values = rdd.reduceByKey(max)\n",
        "\n",
        "results = max_values.collect()\n",
        "for result in results:\n",
        "    print(f\"{result[0]}: {result[1]}\")\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTVa3vn9Bu2j"
      },
      "source": [
        "# Задача 3\n",
        "\n",
        "Напишите программу на Apache Spark, которая считает сумму всех чисел в заданном RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZK_QWNqBvrp",
        "outputId": "17045497-7c53-4e45-9f7f-ea1f775967cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total_sum = 15\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"SumRDDExample\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "data = [1, 2, 3, 4, 5]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "total_sum = rdd.sum()\n",
        "\n",
        "print(\"Total_sum =\", total_sum)\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2viAayBCuW-"
      },
      "source": [
        "# Задача 4\n",
        "\n",
        "У Вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет топ N ключей с наибольшим значением, где N - заданное число."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMDC7k5eCvQ2",
        "outputId": "8269c4a8-f472-4d5a-b0d0-c106f47b2783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key1: 20\n",
            "key2: 15\n",
            "key1: 10\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"TopKeysExample\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "data = [\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 5),\n",
        "    (\"key1\", 20),\n",
        "    (\"key2\", 15),\n",
        "    (\"key3\", 8)\n",
        "]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "N = 3\n",
        "\n",
        "top_N_keys = rdd.takeOrdered(N, key=lambda x: -x[1])\n",
        "\n",
        "for key, value in top_N_keys:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLXEmNUyF5TI"
      },
      "source": [
        "# Задача 5\n",
        "\n",
        "Напишите программу на Apache Spark, которая читает CSV-файл и выводит количество строк в файле."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRT53Y1PF5_f",
        "outputId": "458f809a-40da-409c-8b9a-7f086eb30d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Line count = 5\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"CountCSVLinesExample\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Create data and write it to csv\n",
        "data = [\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 5),\n",
        "    (\"key1\", 20),\n",
        "    (\"key2\", 15),\n",
        "    (\"key3\", 8)\n",
        "]\n",
        "columns = [\"key\", \"value\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.write.csv(\"./data.csv\", header=True)\n",
        "\n",
        "# Get data from csv\n",
        "csv_file_path = \"./data.csv\"\n",
        "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "line_count = df.count()\n",
        "\n",
        "print(\"Line count =\", line_count)\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdpPV_hRICeo"
      },
      "source": [
        "# Задача 6\n",
        "\n",
        "У Вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет сумму значений для каждого уникального ключа, но только для ключей, значение которых больше заданного порога."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Zd_1bvIDjG",
        "outputId": "fbde875c-e226-499c-90fc-e2348eb01e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key1: 30\n",
            "key2: 50\n",
            "key3: 40\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"SumValuesAboveThresholdExample\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "data = [\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 20),\n",
        "    (\"key1\", 30),\n",
        "    (\"key3\", 40),\n",
        "    (\"key2\", 50)\n",
        "]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "threshold = 25\n",
        "\n",
        "filtered_rdd = rdd.filter(lambda x: x[1] > threshold)\n",
        "\n",
        "result = filtered_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "for key, value in result.collect():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задача 7\n",
        "\n",
        "У Вас есть RDD со следующей структурой: (ключ, список значений). Напишите программу на Apache Spark, которая найдет среднюю длину списка для каждого ключа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AverageListLengthExample\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"key1\", [1, 2, 3]),\n",
        "    (\"key2\", [4, 5, 6]),\n",
        "    (\"key1\", [7, 8, 9, 4, 7]),\n",
        "    (\"key2\", [10, 11, 12, 9, 4, 7]),\n",
        "    (\"key3\", [13, 14, 15])\n",
        "]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd_key_length = rdd.mapValues(lambda x: len(x))\n",
        "\n",
        "rdd_sum_count = rdd_key_length.combineByKey(\n",
        "    lambda x: (x, 1),\n",
        "    lambda acc, x: (acc[0] + x, acc[1] + 1),\n",
        "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        ")\n",
        "\n",
        "rdd_avg_length = rdd_sum_count.mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "result = rdd_avg_length.collect()\n",
        "\n",
        "for key, avg_length in result:\n",
        "    print(f\"{key}: {avg_length}\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задача 8\n",
        "\n",
        "У Вас есть RDD, содержащий записи о посещениях веб-сайта со следующей структурой: (пользователь, дата, время). Напишите программу на Apache Spark, которая найдет количество уникальных пользователей для каждой даты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"UniqueUsersPerDataExample\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"user1\", \"2024-04-01\", \"10:00:00\"),\n",
        "    (\"user2\", \"2024-04-01\", \"11:00:00\"),\n",
        "    (\"user3\", \"2024-04-01\", \"12:00:00\"),\n",
        "    (\"user1\", \"2024-04-02\", \"13:00:00\"),\n",
        "    (\"user2\", \"2024-04-02\", \"14:00:00\"),\n",
        "    (\"user3\", \"2024-04-02\", \"15:00:00\"),\n",
        "    (\"user1\", \"2024-04-03\", \"16:00:00\"),\n",
        "]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd_data_user = rdd.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "rdd_unique_users = rdd_data_user.distinct()\n",
        "\n",
        "result = rdd_unique_users.countByKey()\n",
        "\n",
        "for date, count in result.items():\n",
        "    print(f\"{date}: {count}\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задача 9\n",
        "\n",
        "У Вас есть два RDD с одинаковой структурой: (ключ, значение). Напишите программу на Apache Spark, которая выполнит объединение (join) этих RDD по ключу и найдет сумму значений для каждого уникального ключа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDDJoinAndSumExample\").getOrCreate()\n",
        "\n",
        "data1 = [\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 20),\n",
        "    (\"key3\", 30),\n",
        "]\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(data1)\n",
        "\n",
        "data2 = [\n",
        "    (\"key1\", 40),\n",
        "    (\"key2\", 50),\n",
        "    (\"key4\", 60),\n",
        "]\n",
        "\n",
        "rdd2 = spark.sparkContext.parallelize(data2)\n",
        "\n",
        "rdd_joined = rdd1.join(rdd2)\n",
        "\n",
        "rdd_sum = rdd_joined.mapValues(lambda x: x[0] + x[1])\n",
        "\n",
        "result = rdd_sum.collect()\n",
        "\n",
        "for key, value in result:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задача 10\n",
        "\n",
        "Подсчет количества уникальных слов в тексте с использованием DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CountUniqueWordsExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "text = [\n",
        "    \"Hello, Spark\",\n",
        "    \"Hello, World\",\n",
        "    \"Hello, Scala\"\n",
        "    ]\n",
        "\n",
        "textDF = spark.createDataFrame(text, StringType()).toDF(\"text\")\n",
        "wordsDF = textDF.select(explode(split(\"text\", \" \")).alias(\"word\"))\n",
        "\n",
        "result = wordsDF.distinct().count()\n",
        "print(result)\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задача 11\n",
        "\n",
        "Объектно-ориентированный подход с использованием DataFrame API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OOPExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"age\": 35}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df_filtered = df.filter(df.age > 30)\n",
        "df_filtered.show()\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Устанавливаем Scala"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get install -y scala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Код для установки ядра Scala"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install spython-kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Добавляем Scala в список ядер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spython_kernel install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "val spark = SparkSession.builder() \\\n",
        "    .appName(\"Spark SQL basic example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import spark.implicits._\n",
        "\n",
        "val data = Seq(\n",
        "    (\"John\", 30, \"2024-02-29\", true),\n",
        "    (\"Jane\", 35, \"2024-02-28\", false)\n",
        ")\n",
        "\n",
        "val df = data.toDF(\"Name\", \"Age\", \"BirthDate\", \"IsActive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
